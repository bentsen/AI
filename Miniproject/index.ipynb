{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import PyPDF2\n",
    "\n",
    "# Define folder paths\n",
    "Miniproject_dir = 'Miniproject'\n",
    "resources_dir = os.path.join(Miniproject_dir, 'resources')\n",
    "\n",
    "# Create Miniproject folder if it doesn't exist\n",
    "if not os.path.exists(Miniproject_dir):\n",
    "    os.makedirs(Miniproject_dir)\n",
    "\n",
    "# Create resources folder inside Miniproject if it doesn't exist\n",
    "if not os.path.exists(resources_dir):\n",
    "    os.makedirs(resources_dir)\n",
    "\n",
    "# Load data from Wikipedia\n",
    "wiki_url = 'https://en.wikipedia.org/wiki/Healthcare_in_Denmark'\n",
    "wiki_response = requests.get(wiki_url)\n",
    "wiki_soup = BeautifulSoup(wiki_response.text, 'html.parser')\n",
    "wiki_text = wiki_soup.get_text()\n",
    "\n",
    "# Save Wikipedia text to a file\n",
    "wiki_file_path = os.path.join(resources_dir, 'wikipedia_text.txt')\n",
    "with open(wiki_file_path, 'w', encoding='utf-8') as file:\n",
    "    file.write(wiki_text)\n",
    "\n",
    "# Load data from PDF\n",
    "pdf_url = 'https://sum.dk/Media/C/A/Healthcare-in%20denmark%20an%20overview%20english-V16-dec.pdf'\n",
    "pdf_response = requests.get(pdf_url)\n",
    "pdf_file_path = os.path.join(resources_dir, 'healthcare_pdf_text.txt')\n",
    "with open(pdf_file_path, 'wb') as f:\n",
    "    f.write(pdf_response.content)\n",
    "\n",
    "pdf_text = \"\"\n",
    "with open(pdf_file_path, 'rb') as f:\n",
    "    pdf_reader = PyPDF2.PdfReader(f)\n",
    "    for page_num in range(len(pdf_reader.pages)):\n",
    "        pdf_text += pdf_reader.pages[page_num].extract_text()\n",
    "\n",
    "# Save PDF text to a file\n",
    "with open(pdf_file_path, 'w', encoding='utf-8') as file:\n",
    "    file.write(pdf_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "# Text processing\n\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%markdown\n",
    "# Text processing\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/philippesuenson/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/philippesuenson/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/philippesuenson/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed Wikipedia Text:\n",
      "['healthcare', 'denmark', '', 'wikipedia', 'jump', 'content', 'main', 'menu', 'main', 'menu']\n",
      "\n",
      "Preprocessed PDF Text:\n",
      "['', 'healthcare', 'denmark', 'overview', '', 'colophon', 'healthcare', 'denmark', '', 'overview']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download NLTK resources (run only once)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Define folder paths\n",
    "Miniproject_dir = 'Miniproject'\n",
    "resources_dir = os.path.join(Miniproject_dir, 'resources')\n",
    "\n",
    "# Load Wikipedia text data\n",
    "wiki_file_path = os.path.join(resources_dir, 'wikipedia_text.txt')\n",
    "with open(wiki_file_path, 'r', encoding='utf-8') as file:\n",
    "    wiki_text = file.read()\n",
    "\n",
    "# Load PDF text data\n",
    "pdf_file_path = os.path.join(resources_dir, 'healthcare_pdf_text.txt')\n",
    "with open(pdf_file_path, 'r', encoding='utf-8') as file:\n",
    "    pdf_text = file.read()\n",
    "\n",
    "# Text preprocessing\n",
    "def preprocess_text(text):\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Remove punctuation and digits\n",
    "    table = str.maketrans('', '', string.punctuation + string.digits)\n",
    "    tokens = [token.translate(table) for token in tokens]\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "    return tokens\n",
    "\n",
    "# Preprocess Wikipedia text\n",
    "preprocessed_wiki_text = preprocess_text(wiki_text)\n",
    "\n",
    "# Preprocess PDF text\n",
    "preprocessed_pdf_text = preprocess_text(pdf_text)\n",
    "\n",
    "# Display preprocessed text for verification\n",
    "print(\"Preprocessed Wikipedia Text:\")\n",
    "print(preprocessed_wiki_text[:10])  # Displaying first 10 tokens for brevity\n",
    "print(\"\\nPreprocessed PDF Text:\")\n",
    "print(preprocessed_pdf_text[:10])  # Displaying first 10 tokens for brevity\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "# Vectorization\nBag-of-words (BoW)\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%markdown\n",
    "# Vectorization\n",
    "Bag-of-words (BoW)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag-of-Words Matrix Shape (Wikipedia Text): (1, 1064)\n",
      "Bag-of-Words Matrix Shape (PDF Text): (1, 1064)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Initialize CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit and transform the preprocessed text data\n",
    "bow_matrix_wiki = vectorizer.fit_transform([' '.join(preprocessed_wiki_text)])\n",
    "bow_matrix_pdf = vectorizer.transform([' '.join(preprocessed_pdf_text)])\n",
    "\n",
    "# Display the shape of the Bag-of-Words matrix\n",
    "print(\"Bag-of-Words Matrix Shape (Wikipedia Text):\", bow_matrix_wiki.shape)\n",
    "print(\"Bag-of-Words Matrix Shape (PDF Text):\", bow_matrix_pdf.shape)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "TF-IDF (Term Frequency-Inverse Document Frequency)\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%markdown\n",
    "TF-IDF (Term Frequency-Inverse Document Frequency)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Matrix Shape (Wikipedia Text): (1, 1064)\n",
      "TF-IDF Matrix Shape (PDF Text): (1, 1064)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the preprocessed text data\n",
    "tfidf_matrix_wiki = tfidf_vectorizer.fit_transform([' '.join(preprocessed_wiki_text)])\n",
    "tfidf_matrix_pdf = tfidf_vectorizer.transform([' '.join(preprocessed_pdf_text)])\n",
    "\n",
    "# Display the shape of the TF-IDF matrix\n",
    "print(\"TF-IDF Matrix Shape (Wikipedia Text):\", tfidf_matrix_wiki.shape)\n",
    "print(\"TF-IDF Matrix Shape (PDF Text):\", tfidf_matrix_pdf.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "# Interactive application\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%markdown\n",
    "# Interactive application"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain_community'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[26], line 9\u001B[0m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mnltk\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mstem\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m WordNetLemmatizer\n\u001B[1;32m      8\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfeature_extraction\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtext\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m TfidfVectorizer\n\u001B[0;32m----> 9\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mlangchain_community\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mllms\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Ollama\n\u001B[1;32m     10\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mlangchain\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcallbacks\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmanager\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m CallbackManager\n\u001B[1;32m     11\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mlangchain\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcallbacks\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mstreaming_stdout\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m StreamingStdOutCallbackHandler\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'langchain_community'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langdetect import detect\n",
    "\n",
    "\n",
    "\n",
    "# Define folder paths\n",
    "Miniproject_dir = 'Miniproject'\n",
    "resources_dir = os.path.join(Miniproject_dir, 'resources')\n",
    "\n",
    "# Load data from Wikipedia\n",
    "wiki_url = 'https://en.wikipedia.org/wiki/Healthcare_in_Denmark'\n",
    "wiki_response = requests.get(wiki_url)\n",
    "wiki_soup = BeautifulSoup(wiki_response.text, 'html.parser')\n",
    "wiki_text = wiki_soup.get_text()\n",
    "\n",
    "# Load PDF text data\n",
    "pdf_url = 'https://sum.dk/Media/C/A/Healthcare-in%20denmark%20an%20overview%20english-V16-dec.pdf'\n",
    "pdf_response = requests.get(pdf_url)\n",
    "pdf_text = pdf_response.text\n",
    "\n",
    "# Text preprocessing\n",
    "def preprocess_text(text):\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Remove punctuation and digits\n",
    "    tokens = [token for token in tokens if token.isalnum()]\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Preprocess Wikipedia text\n",
    "preprocessed_wiki_text = preprocess_text(wiki_text)\n",
    "\n",
    "# Preprocess PDF text\n",
    "preprocessed_pdf_text = preprocess_text(pdf_text)\n",
    "\n",
    "# Initialize TF-IDF Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the preprocessed text data\n",
    "tfidf_matrix_wiki = tfidf_vectorizer.fit_transform([preprocessed_wiki_text])\n",
    "tfidf_matrix_pdf = tfidf_vectorizer.transform([preprocessed_pdf_text])\n",
    "\n",
    "# Initialize the embeddings model\n",
    "model_name = \"sentence-transformers/paraphrase-MiniLM-L6-v2\"\n",
    "model_kwargs = {\"device\": \"cpu\"}\n",
    "encode_kwargs = {\"normalize_embeddings\": False}\n",
    "embeddings_model = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")\n",
    "\n",
    "# Initialize the Ollama language model\n",
    "llm = Ollama(model=\"mistral\", callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]))\n",
    "\n",
    "# Define a prompt template\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "Use five sentences maximum. Keep the answer as concise as possible.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Helpful Answer:\n",
    "\"\"\"\n",
    "\n",
    "# Create a prompt from the template\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "# Combine the TF-IDF matrices and embeddings model into a single vector\n",
    "combined_vector = tfidf_matrix_wiki + tfidf_matrix_pdf + embeddings_model\n",
    "\n",
    "# Define a function to handle user queries\n",
    "def handle_query(description):\n",
    "    # Detect language and handle non-English inputs\n",
    "    if detect(description) != 'en':\n",
    "        return \"Sorry, I currently support only English descriptions.\"\n",
    "\n",
    "    # Process the description with the retrieval chain\n",
    "    response = llm({\"query\": description, \"context\": combined_vector})\n",
    "    return response[\"result\"]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}