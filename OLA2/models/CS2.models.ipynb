{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "# CS2 model exploration\n\nExploring Machine Learning Models for Predicting Team Favors in CS2 Matches\n\nOur project aims to develop an AI predictor for Counter-Strike 2 (CS2) matches to predict which team holds the advantage based on various in-game factors. These factors include the number of players alive on each team, the types of weapons they possess, the presence of defuse kits, and the remaining time in the round.\n\nWe have collected a comprehensive dataset comprising 122,410 rows and 97 columns, containing information about different rounds of CS2 matches. Our objective is to explore the potential of machine learning algorithms in predicting team favors based on this dataset.\n\nTo achieve this, we are exploring three different machine learning models: logistic regression, random forest, and gradient boosting. Each model offers unique strengths and characteristics, ranging from simplicity and interpretability to high predictive accuracy.\n\nBy experimenting with these models and evaluating their performance using appropriate metrics, such as accuracy, precision, recall, and F1-score, we aim to identify the most suitable approach for predicting team favors in CS2 matches.\n\nThrough this project, we hope to contribute to the development of AI-powered tools that can assist CS2 players and analysts in making informed decisions during matches, ultimately enhancing the overall gaming experience.\n\n\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%markdown\n",
    "# CS2 model exploration\n",
    "\n",
    "Exploring Machine Learning Models for Predicting Team Favors in CS2 Matches\n",
    "\n",
    "Our project aims to develop an AI predictor for Counter-Strike 2 (CS2) matches to predict which team holds the advantage based on various in-game factors. These factors include the number of players alive on each team, the types of weapons they possess, the presence of defuse kits, and the remaining time in the round.\n",
    "\n",
    "We have collected a comprehensive dataset comprising 122,410 rows and 97 columns, containing information about different rounds of CS2 matches. Our objective is to explore the potential of machine learning algorithms in predicting team favors based on this dataset.\n",
    "\n",
    "To achieve this, we are exploring three different machine learning models: logistic regression, random forest, and gradient boosting. Each model offers unique strengths and characteristics, ranging from simplicity and interpretability to high predictive accuracy.\n",
    "\n",
    "By experimenting with these models and evaluating their performance using appropriate metrics, such as accuracy, precision, recall, and F1-score, we aim to identify the most suitable approach for predicting team favors in CS2 matches.\n",
    "\n",
    "Through this project, we hope to contribute to the development of AI-powered tools that can assist CS2 players and analysts in making informed decisions during matches, ultimately enhancing the overall gaming experience.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "# Logistic Regression Model\n\nLogistic Regression is a simple yet powerful algorithm commonly used for binary classification tasks. In this case, we can frame the problem as binary classification where the two classes represent which team is in favor.\nFirst, we need to prepare our dataset by encoding categorical variables such as types of weapons into numerical values. We can also engineer features such as the ratio of players alive on each team, the types of weapons each team has, presence of defuse kits, and the remaining time in the round.\n\nOnce the dataset is prepared, we can train a logistic regression model on the training data. During training, the model will learn the relationship between the features (e.g., number of players alive, types of weapons) and the target variable (which team is in favor).\n\nAfter training, we can evaluate the model's performance using metrics such as accuracy, precision, recall, and F1-score on a separate validation set. We can then use the trained model to predict the favored team in new rounds of CS2 matches.\n\nPros:\n•\tSimplicity and interpretability: Logistic regression provides easily interpretable coefficients for each feature, making it useful for understanding the importance of different factors in predicting the favored team.\n•\tEfficiency: Logistic regression is computationally efficient, particularly with large datasets like yours.\n\nCons:\n•\tLimited flexibility: Logistic regression assumes a linear relationship between the features and the log-odds of the response variable, which may not capture complex interactions in the data.\n\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%markdown\n",
    "# Logistic Regression Model\n",
    "\n",
    "Logistic Regression is a simple yet powerful algorithm commonly used for binary classification tasks. In this case, we can frame the problem as binary classification where the two classes represent which team is in favor.\n",
    "First, we need to prepare our dataset by encoding categorical variables such as types of weapons into numerical values. We can also engineer features such as the ratio of players alive on each team, the types of weapons each team has, presence of defuse kits, and the remaining time in the round.\n",
    "\n",
    "Once the dataset is prepared, we can train a logistic regression model on the training data. During training, the model will learn the relationship between the features (e.g., number of players alive, types of weapons) and the target variable (which team is in favor).\n",
    "\n",
    "After training, we can evaluate the model's performance using metrics such as accuracy, precision, recall, and F1-score on a separate validation set. We can then use the trained model to predict the favored team in new rounds of CS2 matches.\n",
    "\n",
    "Pros:\n",
    "•\tSimplicity and interpretability: Logistic regression provides easily interpretable coefficients for each feature, making it useful for understanding the importance of different factors in predicting the favored team.\n",
    "•\tEfficiency: Logistic regression is computationally efficient, particularly with large datasets like yours.\n",
    "\n",
    "Cons:\n",
    "•\tLimited flexibility: Logistic regression assumes a linear relationship between the features and the log-odds of the response variable, which may not capture complex interactions in the data.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "# Random Forest Model\n\nRandom Forest is an ensemble learning algorithm that constructs multiple decision trees during training and outputs the mode of the classes (classification) or the mean prediction (regression) of the individual trees. It is known for its robustness and ability to handle complex data.\nSimilar to logistic regression, we prepare the dataset by encoding categorical variables and engineering features. We then train a random forest classifier on the training data.\n\nDuring training, each decision tree in the random forest learns different aspects of the data. The final prediction is made by aggregating the predictions of all the trees in the forest.\n\nWe evaluate the performance of the random forest model using the same metrics as before and use it to predict the favored team in new rounds of CS2 matches.\n\nPros:\n•\tFlexibility: Random forests can capture complex non-linear relationships in the data, making them suitable for a wide range of prediction tasks.\n•\tRobustness: Random forests are less prone to overfitting compared to individual decision trees, making them suitable for high-dimensional datasets like yours.\n\nCons:\n•\tLack of interpretability: While random forests provide accurate predictions, interpreting the individual decision trees can be challenging.\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%markdown\n",
    "# Random Forest Model\n",
    "\n",
    "Random Forest is an ensemble learning algorithm that constructs multiple decision trees during training and outputs the mode of the classes (classification) or the mean prediction (regression) of the individual trees. It is known for its robustness and ability to handle complex data.\n",
    "Similar to logistic regression, we prepare the dataset by encoding categorical variables and engineering features. We then train a random forest classifier on the training data.\n",
    "\n",
    "During training, each decision tree in the random forest learns different aspects of the data. The final prediction is made by aggregating the predictions of all the trees in the forest.\n",
    "\n",
    "We evaluate the performance of the random forest model using the same metrics as before and use it to predict the favored team in new rounds of CS2 matches.\n",
    "\n",
    "Pros:\n",
    "•\tFlexibility: Random forests can capture complex non-linear relationships in the data, making them suitable for a wide range of prediction tasks.\n",
    "•\tRobustness: Random forests are less prone to overfitting compared to individual decision trees, making them suitable for high-dimensional datasets like yours.\n",
    "\n",
    "Cons:\n",
    "•\tLack of interpretability: While random forests provide accurate predictions, interpreting the individual decision trees can be challenging.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "# Gradient Boosting Model\n\nGradient Boosting:\nGradient Boosting is another ensemble learning technique that builds a strong predictive model by combining the predictions of multiple weak models, typically decision trees.\nWe follow a similar process as before to prepare the dataset and engineer features. Then, we train a gradient boosting classifier on the training data.\n\nDuring training, the gradient boosting algorithm builds trees sequentially, each one correcting the errors of the previous trees.\n\nWe evaluate the performance of the gradient boosting model and use it for predicting the favored team in new rounds of CS2 matches.\n\nPros:\n•\tHigh predictive accuracy: Gradient boosting often yields high predictive accuracy by sequentially improving the model's performance.\n•\tRobustness: Gradient boosting is robust to outliers and noisy data, which can be beneficial when working with real-world datasets.\n\n\nCons:\n•\tComputationally expensive: Gradient boosting can be computationally expensive, especially with large datasets like yours, due to the sequential nature of the algorithm.\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%markdown\n",
    "# Gradient Boosting Model\n",
    "\n",
    "Gradient Boosting:\n",
    "Gradient Boosting is another ensemble learning technique that builds a strong predictive model by combining the predictions of multiple weak models, typically decision trees.\n",
    "We follow a similar process as before to prepare the dataset and engineer features. Then, we train a gradient boosting classifier on the training data.\n",
    "\n",
    "During training, the gradient boosting algorithm builds trees sequentially, each one correcting the errors of the previous trees.\n",
    "\n",
    "We evaluate the performance of the gradient boosting model and use it for predicting the favored team in new rounds of CS2 matches.\n",
    "\n",
    "Pros:\n",
    "•\tHigh predictive accuracy: Gradient boosting often yields high predictive accuracy by sequentially improving the model's performance.\n",
    "•\tRobustness: Gradient boosting is robust to outliers and noisy data, which can be beneficial when working with real-world datasets.\n",
    "\n",
    "\n",
    "Cons:\n",
    "•\tComputationally expensive: Gradient boosting can be computationally expensive, especially with large datasets like yours, due to the sequential nature of the algorithm.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "# Conclusion\n\nIn conclusion, we have explored three different machine learning models – logistic regression, random forest, and gradient boosting – for predicting the favored team in CS2 matches based on various factors.\n\nGiven the characteristics of our dataset, all three models could potentially perform well.\nIf we valued interpretability as a crucial factor and prefered a simpler model, logistic regression would be a great choice. However do the dynamics of the game, where facts and stats are more important, we have chosen to prioritize predictive accuracy and are willing to sacrifice some interpretability, therefore random forest or gradient boosting would be the best solution for this.\n\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%markdown\n",
    "# Conclusion\n",
    "\n",
    "In conclusion, we have explored three different machine learning models – logistic regression, random forest, and gradient boosting – for predicting the favored team in CS2 matches based on various factors.\n",
    "\n",
    "Given the characteristics of our dataset, all three models could potentially perform well.\n",
    "If we valued interpretability as a crucial factor and prefered a simpler model, logistic regression would be a great choice. However do the dynamics of the game, where facts and stats are more important, we have chosen to prioritize predictive accuracy and are willing to sacrifice some interpretability, therefore random forest or gradient boosting would be the best solution for this.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}